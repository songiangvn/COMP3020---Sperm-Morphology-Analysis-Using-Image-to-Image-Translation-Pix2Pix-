{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnUBHpp4BjhM"
   },
   "source": [
    "## ***Setting up***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T13:08:10.141083Z",
     "iopub.status.busy": "2026-01-14T13:08:10.140805Z",
     "iopub.status.idle": "2026-01-14T13:08:10.146224Z",
     "shell.execute_reply": "2026-01-14T13:08:10.145579Z",
     "shell.execute_reply.started": "2026-01-14T13:08:10.141060Z"
    },
    "id": "aN0t_JycfSut",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import torch # PyTorch\n",
    "import torch.nn as nn # For convenience when defining a model\n",
    "import torchvision # To access pre-trained deep learning models\n",
    "import torchvision.transforms as transforms # For applying pre-processing and data augmentation to the image data\n",
    "from torchvision.datasets import ImageFolder # For defining the storage location of image data\n",
    "from torch.utils.data import TensorDataset # PyTorch uses datasets comprised of tensors\n",
    "from torch.utils.data.dataloader import DataLoader # For loading mini-batches during training process\n",
    "from torch.utils.data import random_split, Dataset, Subset, WeightedRandomSampler # For generating a random split between the training and testing data,\n",
    "#   For creating a dataset object (I think), For taking a subset of a dataset.\n",
    "\n",
    "\n",
    "import numpy as np # For manipulating arrays and other python object types. 'np' is standard shorthand.\n",
    "import pandas as pd # For manipulating dataframes. 'pd' is standard shorthand.\n",
    "from collections import Counter # To count images in each class\n",
    "import matplotlib.pyplot as plt\n",
    "import math # Some basic mathematic functions\n",
    "import warnings # For checking this a working as expected\n",
    "import sklearn\n",
    "import sklearn.metrics # For easy calculation of performance metrics\n",
    "#import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T13:08:10.147634Z",
     "iopub.status.busy": "2026-01-14T13:08:10.147320Z",
     "iopub.status.idle": "2026-01-14T13:08:10.160093Z",
     "shell.execute_reply": "2026-01-14T13:08:10.159431Z",
     "shell.execute_reply.started": "2026-01-14T13:08:10.147615Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_K90AqqEo9QB"
   },
   "source": [
    "##***Functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T13:08:10.161329Z",
     "iopub.status.busy": "2026-01-14T13:08:10.161129Z",
     "iopub.status.idle": "2026-01-14T13:08:10.173969Z",
     "shell.execute_reply": "2026-01-14T13:08:10.173426Z",
     "shell.execute_reply.started": "2026-01-14T13:08:10.161299Z"
    },
    "id": "5dIidbiElNSZ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def foldSizes(dataset, k):\n",
    "  \"\"\"\n",
    "  Takes the dataset and the number of desired folds and determines the number of examples in each fold\n",
    "  \"\"\"\n",
    "  fold_base_size = math.floor(len(dataset)/k) # the minimum size of a fold.\n",
    "  fold_sizes = [fold_base_size]*5 # a list containing the sizes of each fold\n",
    "  # Calculating the un-allocated images\n",
    "  remaining_count = len(dataset) % k\n",
    "  # Distributing the un-allocated images across the folds\n",
    "  for i in range(remaining_count):\n",
    "    fold_sizes[i] += 1\n",
    "\n",
    "  # Checking all images are allocated\n",
    "  if sum(fold_sizes) != len(dataset):\n",
    "    warnings.warn(\"Not all iamges are allocated to folds\")\n",
    "\n",
    "  return fold_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T13:08:10.174884Z",
     "iopub.status.busy": "2026-01-14T13:08:10.174658Z",
     "iopub.status.idle": "2026-01-14T13:08:10.191791Z",
     "shell.execute_reply": "2026-01-14T13:08:10.191186Z",
     "shell.execute_reply.started": "2026-01-14T13:08:10.174866Z"
    },
    "id": "ZXJwVQRk8XZk",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def allFoldsTrainTestIndices(seed, dataset, fold_sizes):\n",
    "  \"\"\"\n",
    "  Outputs a list containing the train and test set indices for each of the k-folds, given a random seed.\n",
    "  This can be used for completing the 3x 5-fold CV in parts rather than all at once.\n",
    "  \"\"\"\n",
    "  # Set seed for random split\n",
    "  torch.manual_seed(seed)\n",
    "\n",
    "  # Create the indices for each fold\n",
    "  folds = torch.utils.data.random_split(dataset, fold_sizes)\n",
    "  # Extracting indices\n",
    "  fold_indices = []\n",
    "  for fold in folds:\n",
    "    fold_indices.append(fold.indices)\n",
    "\n",
    "  # Creating train and test sets (ie. combining indices) for each fold\n",
    "\n",
    "  train_test_idx_folds = [] # Storage containing train and test indices for all folds\n",
    "\n",
    "  for fold in range(len(fold_sizes)): # len(fold_sizes) is effectively 'k'\n",
    "    train_idx = [] # Storage\n",
    "    test_idx = []  # Storage\n",
    "    train_test_idx_fold = [] # Storage for both the above lists\n",
    "\n",
    "    # Using respective folds to create train and test sets\n",
    "    for i in range(k):\n",
    "      if i == fold:\n",
    "        test_idx = fold_indices[i]\n",
    "      else:\n",
    "        train_idx = train_idx + fold_indices[i]\n",
    "\n",
    "      # Checking that there is no overlap\n",
    "    if len(set(train_idx).intersection(test_idx)): # If there is any overlap between the training and testing sets\n",
    "      warnings.warn(\"Train and test set images are not mutually exclusive\")\n",
    "\n",
    "    # Making a list of lists containing train indicies and test indices\n",
    "    train_test_idx_fold.append(train_idx)\n",
    "    train_test_idx_fold.append(test_idx)\n",
    "\n",
    "    # Append train and test indices for this fold to a list containing this for all folds\n",
    "    train_test_idx_folds.append(train_test_idx_fold)\n",
    "  return train_test_idx_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T13:08:10.260377Z",
     "iopub.status.busy": "2026-01-14T13:08:10.259870Z",
     "iopub.status.idle": "2026-01-14T13:08:10.270056Z",
     "shell.execute_reply": "2026-01-14T13:08:10.269580Z",
     "shell.execute_reply.started": "2026-01-14T13:08:10.260359Z"
    },
    "id": "rGOp5bTjDAXC",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def trainBaseModel(model, num_epochs, learn_rate, dl_train, dl_test,\n",
    "                   patience=20, min_delta=1e-4):\n",
    "  \"\"\"\n",
    "  Training loop with early stopping based on F1-score.\n",
    "  NOTE: Early stopping is performed on test data (not ideal).\n",
    "  \"\"\"\n",
    "\n",
    "  device = torch.device('cuda:0')\n",
    "  model.to(device)\n",
    "\n",
    "  best_f1 = 0\n",
    "  best_acc = 0\n",
    "  best_re = 0\n",
    "  best_pr = 0\n",
    "  best_epoch = 0\n",
    "\n",
    "  f1_scores = []\n",
    "  epochs_no_improve = 0\n",
    "\n",
    "  optimizer = torch.optim.Adam(params=model.parameters(), lr=learn_rate)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "\n",
    "    # -----------------------\n",
    "    # TRAIN\n",
    "    # -----------------------\n",
    "    model.train()\n",
    "    for images, labels in dl_train:\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      outputs = model(images)\n",
    "      loss = criterion(outputs, labels)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    # -----------------------\n",
    "    # EVALUATE\n",
    "    # -----------------------\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    test_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for images, labels in dl_test:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_preds += predicted.tolist()\n",
    "        test_labels += labels.tolist()\n",
    "\n",
    "    test_preds = torch.tensor(test_preds).cpu()\n",
    "    test_labels = torch.tensor(test_labels).cpu()\n",
    "\n",
    "    accuracy = sklearn.metrics.accuracy_score(test_labels, test_preds)\n",
    "    recall = sklearn.metrics.recall_score(test_labels, test_preds, average=\"macro\")\n",
    "    precision = sklearn.metrics.precision_score(test_labels, test_preds, average=\"macro\")\n",
    "    f1_score = sklearn.metrics.f1_score(test_labels, test_preds, average=\"macro\")\n",
    "\n",
    "    f1_scores.append(f1_score)\n",
    "\n",
    "    # -----------------------\n",
    "    # EARLY STOPPING CHECK\n",
    "    # -----------------------\n",
    "    if f1_score > best_f1 + min_delta:\n",
    "      best_f1 = f1_score\n",
    "      best_acc = accuracy\n",
    "      best_re = recall\n",
    "      best_pr = precision\n",
    "      best_epoch = epoch\n",
    "      epochs_no_improve = 0\n",
    "\n",
    "      torch.save(model, '/kaggle/working/model.pth')\n",
    "\n",
    "    else:\n",
    "      epochs_no_improve += 1\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "      print(f\"Epoch [{epoch+1}/{num_epochs}] | F1: {f1_score:.4f}\")\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "      print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "      break\n",
    "\n",
    "  # -----------------------\n",
    "  # RESULTS\n",
    "  # -----------------------\n",
    "  print('Results:\\n \\\n",
    "  Accuracy  = {:.3f}% \\n \\\n",
    "  Recall    = {:.3f}% \\n \\\n",
    "  Precision = {:.3f}% \\n \\\n",
    "  F1 Score  = {:.3f}% \\n \\\n",
    "  Best Epoch = {}'.format(\n",
    "      best_acc, best_re, best_pr, best_f1, best_epoch + 1\n",
    "  ))\n",
    "\n",
    "  plt.plot(f1_scores)\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"F1-score\")\n",
    "  plt.show()\n",
    "\n",
    "  return best_acc, best_re, best_pr, best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T13:08:10.271512Z",
     "iopub.status.busy": "2026-01-14T13:08:10.271251Z",
     "iopub.status.idle": "2026-01-14T13:08:10.287379Z",
     "shell.execute_reply": "2026-01-14T13:08:10.286887Z",
     "shell.execute_reply.started": "2026-01-14T13:08:10.271494Z"
    },
    "id": "UcyauXfKbY4o",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def trainMetaClassifier(model, num_epochs, learn_rate, momentum, weight_decay,\n",
    "                        dl_train, dl_test, patience=10, min_delta=1e-4):\n",
    "  \"\"\"\n",
    "  Training loop for the Meta-Classifier with early stopping.\n",
    "  NOTE: Early stopping is performed on test data (not ideal).\n",
    "  \"\"\"\n",
    "\n",
    "  device = torch.device('cuda:0')\n",
    "  model.to(device)\n",
    "\n",
    "  best_f1 = 0\n",
    "  best_acc = 0\n",
    "  best_re = 0\n",
    "  best_pr = 0\n",
    "  best_epoch = 0\n",
    "\n",
    "  epochs_no_improve = 0\n",
    "\n",
    "  optimizer = torch.optim.SGD(\n",
    "      params=model.parameters(),\n",
    "      lr=learn_rate,\n",
    "      momentum=momentum,\n",
    "      weight_decay=weight_decay\n",
    "  )\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "\n",
    "    # -----------------------\n",
    "    # TRAIN\n",
    "    # -----------------------\n",
    "    model.train()\n",
    "    for inputs, labels in dl_train:\n",
    "      inputs = inputs.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      outputs = model(inputs)\n",
    "      loss = criterion(outputs, labels)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    # -----------------------\n",
    "    # EVALUATE\n",
    "    # -----------------------\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    test_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for inputs, labels in dl_test:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_preds += predicted.tolist()\n",
    "        test_labels += labels.tolist()\n",
    "\n",
    "    test_preds = torch.tensor(test_preds).cpu()\n",
    "    test_labels = torch.tensor(test_labels).cpu()\n",
    "\n",
    "    accuracy = sklearn.metrics.accuracy_score(test_labels, test_preds)\n",
    "    recall = sklearn.metrics.recall_score(test_labels, test_preds, average=\"macro\")\n",
    "    precision = sklearn.metrics.precision_score(test_labels, test_preds, average=\"macro\")\n",
    "    f1_score = sklearn.metrics.f1_score(test_labels, test_preds, average=\"macro\")\n",
    "\n",
    "    # -----------------------\n",
    "    # EARLY STOPPING CHECK\n",
    "    # -----------------------\n",
    "    if f1_score > best_f1 + min_delta:\n",
    "      best_f1 = f1_score\n",
    "      best_acc = accuracy\n",
    "      best_re = recall\n",
    "      best_pr = precision\n",
    "      best_epoch = epoch\n",
    "      epochs_no_improve = 0\n",
    "\n",
    "      torch.save(model, '/kaggle/working/model.pth')\n",
    "\n",
    "    else:\n",
    "      epochs_no_improve += 1\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "      print(f\"Epoch [{epoch+1}/{num_epochs}] | F1: {f1_score:.4f}\")\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "      print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "      break\n",
    "\n",
    "  # -----------------------\n",
    "  # RESULTS\n",
    "  # -----------------------\n",
    "  print('Results:\\n \\\n",
    "  Accuracy  = {:.3f}% \\n \\\n",
    "  Recall    = {:.3f}% \\n \\\n",
    "  Precision = {:.3f}% \\n \\\n",
    "  F1 Score  = {:.3f}% \\n \\\n",
    "  Best Epoch = {}'.format(\n",
    "      best_acc, best_re, best_pr, best_f1, best_epoch + 1\n",
    "  ))\n",
    "\n",
    "  return best_acc, best_re, best_pr, best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T13:08:10.288251Z",
     "iopub.status.busy": "2026-01-14T13:08:10.288066Z",
     "iopub.status.idle": "2026-01-14T13:08:10.305347Z",
     "shell.execute_reply": "2026-01-14T13:08:10.304769Z",
     "shell.execute_reply.started": "2026-01-14T13:08:10.288235Z"
    },
    "id": "FcB_H9O_OnIn",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "  # Softmax function\n",
    "def softmax(x):\n",
    "  return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T13:08:10.307160Z",
     "iopub.status.busy": "2026-01-14T13:08:10.306967Z",
     "iopub.status.idle": "2026-01-14T13:08:10.322358Z",
     "shell.execute_reply": "2026-01-14T13:08:10.321780Z",
     "shell.execute_reply.started": "2026-01-14T13:08:10.307143Z"
    },
    "id": "y23xTOOA8793",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Generate predictions using the trained model\n",
    "def generateCNNPredictions(data_path, transformations, train_test_idx, batch_size, seed, device='cuda:0'):\n",
    "  \"\"\"\n",
    "  This function produces predictions for a single fold using the trained model saved to the temporary location.\n",
    "  These will form part of the train and test sets for the meta-classifier.\n",
    "  \"\"\"\n",
    "\n",
    "  # Load the data with test set transformations applied (ie. no data augmentation)\n",
    "  dataset = ImageFolder(data_path, transform=transformations)\n",
    "  train_test_idx[0].sort() # Sorting in place, not required.\n",
    "  train_test_idx[1].sort() # ^\n",
    "  ds_train = Subset(dataset, indices=train_test_idx[0])\n",
    "  ds_test = Subset(dataset, indices=train_test_idx[1])\n",
    "\n",
    "  # We need to redefine the train set sampler to ensure all base-models predict on the data in the same order\n",
    "  labels_train = [dataset.targets[i] for i in train_test_idx[0]]\n",
    "  counts = dict(Counter(labels_train))\n",
    "  counts = np.array(list(counts.values())) # converting counts to a numpy array\n",
    "  # Assigning a sampling weight to each class\n",
    "  weights = 1. / counts\n",
    "  # Getting weights for each image (ie. weight for the class that each image belongs to)\n",
    "  samples_weight = np.array([weights[t] for t in labels_train])\n",
    "  samples_weight = torch.from_numpy(samples_weight) # Converting to a tensor\n",
    "  samples_weight = samples_weight.double() # Converting elements to doubles for some reason\n",
    "  # Defining the sampler\n",
    "  g = torch.Generator()\n",
    "  g.manual_seed(seed)\n",
    "  sampler = WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True, generator=g)\n",
    "  # Now defining the dataloaders\n",
    "  dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=False, sampler=sampler)\n",
    "  dl_test = DataLoader(ds_test, batch_size=batch_size, shuffle=False)\n",
    "  # Loading the model from save\n",
    "  model = torch.load('/kaggle/working/model.pth', weights_only=False)\n",
    "  # Sending to device for faster computing\n",
    "  model.to(device)\n",
    "\n",
    "  # Generating predictions\n",
    "\n",
    "  # Train set\n",
    "  # Storage\n",
    "  preds_train = []\n",
    "  labels_train = []\n",
    "  model.eval() # Set model to evaluation mode, we aren't training so we want the best predictions possible\n",
    "  with torch.no_grad(): # As we aren't performing backprop, we don't need gradients\n",
    "    for i, (images, labels) in enumerate(dl_train):\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "      preds = model(images)\n",
    "      preds_train = preds_train + preds.tolist()\n",
    "      labels_train = labels_train + labels.tolist()\n",
    "\n",
    "  # Test set\n",
    "  # Storage\n",
    "  preds_test = []\n",
    "  labels_test = []\n",
    "  model.eval() # Set model to evaluation mode, we aren't training so we want the best predictions possible\n",
    "  with torch.no_grad(): # As we aren't performing backprop, we don't need gradients\n",
    "    for i, (images, labels) in enumerate(dl_test):\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "      preds = model(images)\n",
    "      preds_test = preds_test + preds.tolist()\n",
    "      labels_test = labels_test + labels.tolist()\n",
    "\n",
    "  # Converting to softmax\n",
    "  # Train\n",
    "  for i in range(len(preds_train)):\n",
    "    preds_train[i] = softmax(preds_train[i])\n",
    "  # Test\n",
    "  for i in range(len(preds_test)):\n",
    "    preds_test[i] = softmax(preds_test[i])\n",
    "\n",
    "  # Convert to dataframes for easier saving\n",
    "  df_train = pd.merge(pd.DataFrame(labels_train), pd.DataFrame(preds_train), left_index=True, right_index=True)\n",
    "  df_test = pd.merge(pd.DataFrame(labels_test), pd.DataFrame(preds_test), left_index=True, right_index=True)\n",
    "  # Adjusting column names\n",
    "  col_names = ['labels', '0', '1', '2', '3']\n",
    "  df_train.columns = col_names\n",
    "  df_test.columns = col_names\n",
    "\n",
    "  return df_train, df_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T13:08:10.323343Z",
     "iopub.status.busy": "2026-01-14T13:08:10.323070Z",
     "iopub.status.idle": "2026-01-14T13:08:10.340570Z",
     "shell.execute_reply": "2026-01-14T13:08:10.340103Z",
     "shell.execute_reply.started": "2026-01-14T13:08:10.323326Z"
    },
    "id": "t8SKsInq1E0G",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generateMCPredictions(dl_train, dl_test, batch_size, device='cuda:0'):\n",
    "  \"\"\"\n",
    "  Generates predictions using the trained and temporarily saved meta-classifier\n",
    "  \"\"\"\n",
    "    # Loading the model from save\n",
    "  model = torch.load('/kaggle/working/model.pth', weights_only=False)\n",
    "  # Sending to device for faster computing\n",
    "  model.to(device)\n",
    "\n",
    "  # Generating predictions\n",
    "  #\n",
    "  # Train set\n",
    "  # Storage\n",
    "  preds_train = []\n",
    "  labels_train = []\n",
    "  model.eval() # Set model to evaluation mode, we aren't training so we want the best predictions possible\n",
    "  with torch.no_grad(): # As we aren't performing backprop, we don't need gradients\n",
    "    for i, (inputs, targets) in enumerate(dl_train):\n",
    "      inputs = inputs.to(device)\n",
    "      targets = targets.to(device)\n",
    "      preds = model(inputs)\n",
    "      preds_train = preds_train + preds.tolist()\n",
    "      labels_train = labels_train + targets.tolist()\n",
    "\n",
    "  # Test set\n",
    "  # Storage\n",
    "  preds_test = []\n",
    "  labels_test = []\n",
    "  model.eval() # Set model to evaluation mode, we aren't training so we want the best predictions possible\n",
    "  with torch.no_grad(): # As we aren't performing backprop, we don't need gradients\n",
    "    for i, (inputs, targets) in enumerate(dl_test):\n",
    "      inputs = inputs.to(device)\n",
    "      targets = targets.to(device)\n",
    "      preds = model(inputs)\n",
    "      preds_test = preds_test + preds.tolist()\n",
    "      labels_test = labels_test + targets.tolist()\n",
    "\n",
    "  # Converting to softmax\n",
    "  # Train\n",
    "  for i in range(len(preds_train)):\n",
    "    preds_train[i] = softmax(preds_train[i])\n",
    "  # Test\n",
    "  for i in range(len(preds_test)):\n",
    "    preds_test[i] = softmax(preds_test[i])\n",
    "\n",
    "  # Convert to dataframes for easier saving\n",
    "  df_train = pd.merge(pd.DataFrame(labels_train), pd.DataFrame(preds_train), left_index=True, right_index=True)\n",
    "  df_test = pd.merge(pd.DataFrame(labels_test), pd.DataFrame(preds_test), left_index=True, right_index=True)\n",
    "  # Adjusting column names\n",
    "  col_names = ['labels', '0', '1', '2', '3']\n",
    "  df_train.columns = col_names\n",
    "  df_test.columns = col_names\n",
    "\n",
    "  return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T13:08:10.341672Z",
     "iopub.status.busy": "2026-01-14T13:08:10.341427Z",
     "iopub.status.idle": "2026-01-14T13:08:10.356555Z",
     "shell.execute_reply": "2026-01-14T13:08:10.356026Z",
     "shell.execute_reply.started": "2026-01-14T13:08:10.341654Z"
    },
    "id": "bRDJ203rX4R_",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Identity class for modifying ResNet-34\n",
    "#   Setting any layer to this class will effectively remove the layer.\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T13:08:10.357406Z",
     "iopub.status.busy": "2026-01-14T13:08:10.357207Z",
     "iopub.status.idle": "2026-01-14T13:08:10.376112Z",
     "shell.execute_reply": "2026-01-14T13:08:10.375488Z",
     "shell.execute_reply.started": "2026-01-14T13:08:10.357379Z"
    },
    "id": "_iUPr1tygGs8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def getModel(model_name):\n",
    "    \"\"\"\n",
    "    Loads and returns a pre-trained CNN\n",
    "    \"\"\"\n",
    "    pretrained = True\n",
    "    num_classes=4\n",
    "    \n",
    "    if model_name == 'vgg16':\n",
    "        model = torchvision.models.vgg16(pretrained=pretrained)\n",
    "        model.classifier[6] = nn.Linear(in_features=4096, out_features=num_classes, bias=True)\n",
    "    elif model_name == 'vgg19':\n",
    "        model = torchvision.models.vgg19(pretrained=pretrained)\n",
    "        model.classifier[6] = nn.Linear(in_features=4096, out_features=num_classes, bias=True)\n",
    "    elif model_name == 'resnet':\n",
    "        model = torchvision.models.resnet34(pretrained=pretrained)\n",
    "        model.fc = nn.Linear(in_features=256, out_features=num_classes, bias=True)\n",
    "        model.layer4 = Identity()\n",
    "    elif model_name == 'densenet':\n",
    "        model = torchvision.models.densenet161(pretrained=pretrained)\n",
    "        model.classifier = nn.Linear(in_features=2208, out_features=num_classes, bias=True)\n",
    "    \n",
    "    # Returning the required model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T13:08:10.376970Z",
     "iopub.status.busy": "2026-01-14T13:08:10.376804Z",
     "iopub.status.idle": "2026-01-14T13:08:10.385347Z",
     "shell.execute_reply": "2026-01-14T13:08:10.384902Z",
     "shell.execute_reply.started": "2026-01-14T13:08:10.376955Z"
    },
    "id": "wydGmNmfoFFJ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_name_generator(cv_config):\n",
    "    \"\"\"\n",
    "    Generates the path to save the generated predictions to.\n",
    "    \"\"\"\n",
    "    # cv_config is a dataframe row containing the info for the model fit that was just done\n",
    "    model_name, repeat, fold, = cv_config['model'], cv_config['repeat'], cv_config['fold']\n",
    "    path_base = '/kaggle/working/'\n",
    "    repeats = ['repeat 0/', 'repeat 1/', 'repeat 2/']\n",
    "    # repeats = ['repeat 0/']\n",
    "    folds = ['fold 0/', 'fold 1/', 'fold 2/', 'fold 3/', 'fold 4/']\n",
    "    repeat = repeats[repeat]\n",
    "    fold = folds[fold]\n",
    "    train_path = path_base + repeat + fold + model_name + '_train.csv'\n",
    "    test_path = path_base + repeat + fold + model_name + '_test.csv'\n",
    "    os.makedirs(os.path.dirname(train_path), exist_ok=True)  # Create directory for train_path\n",
    "    os.makedirs(os.path.dirname(test_path), exist_ok=True)   # Create directory for test_path\n",
    "    return train_path, test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T13:08:10.387398Z",
     "iopub.status.busy": "2026-01-14T13:08:10.387069Z",
     "iopub.status.idle": "2026-01-14T13:08:10.402742Z",
     "shell.execute_reply": "2026-01-14T13:08:10.402115Z",
     "shell.execute_reply.started": "2026-01-14T13:08:10.387380Z"
    },
    "id": "smmAlCRYtb6Z",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def MyDataset(path):\n",
    "  \"\"\"\n",
    "  For preparing meta-classifier input data to use with PyTorch.\n",
    "  \"\"\"\n",
    "  # Load dataframe from CSV file\n",
    "  df = pd.read_csv(path)\n",
    "  # Define inputs and outputs\n",
    "  df_inputs = df.iloc[:,1:] # Select all columns except first (labels col)\n",
    "  df_targets = df.iloc[:,0] # Select first column\n",
    "  # Performing a necessary conversion from dataframe object\n",
    "  inputs = torch.from_numpy(df_inputs.values)\n",
    "  targets = torch.from_numpy(df_targets.values)\n",
    "  # Converting to a dataset object\n",
    "  ds = TensorDataset(inputs.float(), targets)\n",
    "  # Return the final dataset object we wanted\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T13:08:10.404247Z",
     "iopub.status.busy": "2026-01-14T13:08:10.403633Z",
     "iopub.status.idle": "2026-01-14T13:08:10.418933Z",
     "shell.execute_reply": "2026-01-14T13:08:10.418443Z",
     "shell.execute_reply.started": "2026-01-14T13:08:10.404223Z"
    },
    "id": "OvdwnPlQyw-K",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# The meta-classifier class\n",
    "class metaClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "      super(metaClassifier, self).__init__()\n",
    "      # Here we define the layers\n",
    "      self.fc1 = nn.Linear(20, 32) # The input layer. It takes 20 inputs, and outputs 32\n",
    "      self.bn1 = nn.BatchNorm1d(32)\n",
    "      self.dropout1 = nn.Dropout(0.2)\n",
    "      self.fc2 = nn.Linear(32, 32) # Takes the 32 outputted from fc1\n",
    "      self.bn2 = nn.BatchNorm1d(32)\n",
    "      self.dropout2 = nn.Dropout(0.2)\n",
    "      self.fc3 = nn.Linear(32, 5) # Takes the 32 from fc2, outputs our 5 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "      # Here we define how the data will pass through the layers\n",
    "      x = self.fc1(x)\n",
    "      x = self.bn1(x)\n",
    "      x = self.dropout1(x)\n",
    "      x = self.fc2(x)\n",
    "      x = self.bn2(x)\n",
    "      x = self.dropout2(x)\n",
    "      out = self.fc3(x)\n",
    "      return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T13:08:10.420174Z",
     "iopub.status.busy": "2026-01-14T13:08:10.419766Z",
     "iopub.status.idle": "2026-01-14T13:08:10.437028Z",
     "shell.execute_reply": "2026-01-14T13:08:10.436347Z",
     "shell.execute_reply.started": "2026-01-14T13:08:10.420156Z"
    },
    "id": "L89BejrAqgR0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def getSampler(subset_train):\n",
    "  \"\"\"Generates the weighted random sampler using the train subset as input\"\"\"\n",
    "  # Getting train set indices\n",
    "  # Getting labels after sorting indices so weights are applied correctly\n",
    "  subset_train.indices.sort()\n",
    "  labels_train = [subset_train.dataset.targets[i] for i in subset_train.indices]\n",
    "\n",
    "  # Counting\n",
    "  counts = dict(Counter(labels_train))\n",
    "  counts = np.array(list(counts.values())) # converting counts to a numpy array\n",
    "\n",
    "  # Assigning a sampling weight to each class\n",
    "  weights = 1. / counts\n",
    "\n",
    "  # Getting/assigning weights for each image (ie. weight for the class that each image belongs to)\n",
    "  samples_weight = np.array([weights[t] for t in labels_train])\n",
    "  samples_weight = torch.from_numpy(samples_weight) # Converting to a tensor\n",
    "  samples_weight = samples_weight.double() # Convrting elements to doubles for some reason\n",
    "\n",
    "  # Defining the sampler\n",
    "  sampler = WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "  return sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5DiOOuGBxz9"
   },
   "source": [
    "## ***Base-Models***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformSubset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.subset[idx]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T13:08:10.438412Z",
     "iopub.status.busy": "2026-01-14T13:08:10.437960Z",
     "iopub.status.idle": "2026-01-14T13:08:18.525289Z",
     "shell.execute_reply": "2026-01-14T13:08:18.524205Z",
     "shell.execute_reply.started": "2026-01-14T13:08:10.438393Z"
    },
    "id": "G_AopmiOHE9o",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score  # Example metric for evaluation\n",
    "from collections import defaultdict\n",
    "\n",
    "# Data preparation\n",
    "\n",
    "# Defining the transformations\n",
    "transformations_train = transforms.Compose([\n",
    "    transforms.CenterCrop(70),  # Crop the center to 70x70\n",
    "    # transforms.Resize(224),\n",
    "    transforms.RandomVerticalFlip(0.5),\n",
    "    # transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transformations_test = transforms.Compose([\n",
    "    transforms.CenterCrop(70),  # Crop the center to 70x70\n",
    "    # transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Loading the image datasets\n",
    "data_dir_real = '/kaggle/input/sperm-datasets/Stained/HuSHem'\n",
    "data_dir_syn  = '/kaggle/input/sperm-datasets/Stained/HuSHem_synthetic'\n",
    "dataset_real_train = ImageFolder(data_dir_real, transform=transformations_train)\n",
    "dataset_real_test  = ImageFolder(data_dir_real, transform=transformations_test)\n",
    "dataset_syn = ImageFolder(data_dir_syn, transform=None)\n",
    "\n",
    "# Generate fold sizes and the indices for K-fold CV (since you're not using CSV)\n",
    "k = 5  # Number of folds for K-fold CV\n",
    "fold_sizes = foldSizes(dataset_real_train, k)  # Number of images in each fold\n",
    "\n",
    "# Getting train-test indices for all folds and repeats\n",
    "seeds = [0, 69, 420]  # The random split is dependent on the seeds chosen.\n",
    "train_test_idx_all = []  # Storage for train-test indices\n",
    "\n",
    "# synthetic_lookup[class_idx][filename] = synthetic_dataset_index\n",
    "synthetic_lookup = defaultdict(dict)\n",
    "\n",
    "for idx, (path, label) in enumerate(dataset_syn.samples):\n",
    "    fname = os.path.basename(path)\n",
    "    synthetic_lookup[label][fname] = idx\n",
    "\n",
    "# Generate indices for each seed and store them\n",
    "for seed in seeds:\n",
    "    train_test_idx_all = train_test_idx_all + allFoldsTrainTestIndices(seed, dataset_real_train, fold_sizes)\n",
    "\n",
    "# train_test_idx_all is of length 15 (3 x 5-fold CV). To easily iterate over it in the loop below, let's put 4 train_test_idx_all's together.\n",
    "train_test_idx_all = train_test_idx_all\n",
    "\n",
    "# Defining hyperparameters for each base-model\n",
    "hyperparameters_all = [[100, 1e-4, 32], [100, 1e-4, 32], [100, 1e-4, 32], [100, 1e-4, 32]]\n",
    "model_names = ['vgg16', 'vgg19', 'resnet', 'densenet']\n",
    "\n",
    "\n",
    "# Loop over each model\n",
    "for model_name in model_names:\n",
    "    metrics = []\n",
    "    # Get the corresponding hyperparameters\n",
    "    model_hyperparameters = hyperparameters_all[model_names.index(model_name)]\n",
    "    \n",
    "    # Loop through train-test splits for each seed and fold\n",
    "    for idx, train_test_idx in enumerate(train_test_idx_all):\n",
    "        # Create DataLoader for training and testing\n",
    "        # -----------------------\n",
    "        # REAL TRAIN / TEST\n",
    "        # -----------------------\n",
    "        real_train_idx = train_test_idx[0]\n",
    "        real_test_idx  = train_test_idx[1]\n",
    "        \n",
    "        # -----------------------\n",
    "        # MATCHED SYNTHETIC TRAIN\n",
    "        # -----------------------\n",
    "        syn_train_idx = []\n",
    "        \n",
    "        for real_idx in real_train_idx:\n",
    "            real_path, real_label = dataset_real_train.samples[real_idx]\n",
    "            fname = os.path.basename(real_path)\n",
    "        \n",
    "            if fname in synthetic_lookup[real_label]:\n",
    "                syn_idx = synthetic_lookup[real_label][fname]\n",
    "                syn_train_idx.append(syn_idx)\n",
    "        \n",
    "        # -----------------------\n",
    "        # FINAL TRAIN DATASET\n",
    "        # -----------------------\n",
    "        ds_train_real = Subset(dataset_real_train, real_train_idx)\n",
    "        ds_train_syn_raw = Subset(dataset_syn, syn_train_idx)\n",
    "        \n",
    "        ds_train_syn = TransformSubset(\n",
    "            ds_train_syn_raw,\n",
    "            transform=transformations_train\n",
    "        )\n",
    "        \n",
    "        ds_train = torch.utils.data.ConcatDataset([\n",
    "            ds_train_real,\n",
    "            ds_train_syn\n",
    "        ])\n",
    "\n",
    "        # ds_train = Subset(dataset_real_train, real_train_idx)\n",
    "\n",
    "        # -----------------------\n",
    "        # TEST DATASET (REAL ONLY)\n",
    "        # -----------------------\n",
    "        ds_test = Subset(dataset_real_test, real_test_idx)\n",
    "        \n",
    "        dl_train = DataLoader(ds_train, batch_size=model_hyperparameters[2], shuffle=True)\n",
    "        dl_test = DataLoader(ds_test, batch_size=len(ds_test), shuffle=False)\n",
    "\n",
    "        print(f\"Fold {idx}: real train = {len(real_train_idx)}, synthetic added = {len(syn_train_idx)}\")\n",
    "        \n",
    "        # Get the model\n",
    "        model = getModel(model_name)\n",
    "        \n",
    "        # Training\n",
    "        best_acc, best_re, best_pr, best_f1 = trainBaseModel(model=model,\n",
    "                       num_epochs=model_hyperparameters[0],\n",
    "                       learn_rate=model_hyperparameters[1],\n",
    "                       dl_train=dl_train,\n",
    "                       dl_test=dl_test)\n",
    "    \n",
    "        # Use idx % len(seeds) to cycle through the seeds list if it exceeds the length of seeds\n",
    "        seed = seeds[idx % len(seeds)]  # Cycle through seeds\n",
    "    \n",
    "        # Generate predictions using the trained model\n",
    "        df_preds_train, df_preds_test = generateCNNPredictions(data_path=data_dir_real,\n",
    "                                                              transformations=transformations_test,\n",
    "                                                              train_test_idx=train_test_idx,\n",
    "                                                              batch_size=model_hyperparameters[2],\n",
    "                                                              seed=seed)  # Use the corresponding seed\n",
    "    \n",
    "        # Save predictions using the save_name_generator function, passing the cv_config for this run\n",
    "        cv_config = {\"model\": model_name, \"repeat\": idx // 5, \"fold\": idx % 5}  # Create the config for this fold\n",
    "        train_path, test_path = save_name_generator(cv_config)  # Pass the config here\n",
    "    \n",
    "        # Save predictions\n",
    "        df_preds_train.to_csv(train_path, index=False)\n",
    "        df_preds_test.to_csv(test_path, index=False)\n",
    "    \n",
    "        metrics.append((best_acc, best_re, best_pr, best_f1))\n",
    "    \n",
    "        # Print progress\n",
    "        print(f\"Completed training and prediction for {model_name}, fold {idx + 1}, accuracy: {best_acc}\")\n",
    "    \n",
    "    metrics = np.array(metrics)\n",
    "    \n",
    "    mean_acc = metrics[:, 0].mean()\n",
    "    std_acc  = metrics[:, 0].std(ddof=1)\n",
    "    \n",
    "    mean_re  = metrics[:, 1].mean()\n",
    "    std_re   = metrics[:, 1].std(ddof=1)\n",
    "    \n",
    "    mean_pr  = metrics[:, 2].mean()\n",
    "    std_pr   = metrics[:, 2].std(ddof=1)\n",
    "    \n",
    "    mean_f1  = metrics[:, 3].mean()\n",
    "    std_f1   = metrics[:, 3].std(ddof=1)\n",
    "    \n",
    "    print(f\"Accuracy : {mean_acc*100:.2f} ± {std_acc*100:.2f}\")\n",
    "    print(f\"Recall   : {mean_re*100:.2f} ± {std_re*100:.2f}\")\n",
    "    print(f\"Precision: {mean_pr*100:.2f} ± {std_pr*100:.2f}\")\n",
    "    print(f\"F1-score : {mean_f1*100:.2f} ± {std_f1*100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crfOBNfWe-N6"
   },
   "source": [
    "## ***Meta-Classifier***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-14T13:08:18.526192Z",
     "iopub.status.idle": "2026-01-14T13:08:18.526572Z",
     "shell.execute_reply": "2026-01-14T13:08:18.526395Z",
     "shell.execute_reply.started": "2026-01-14T13:08:18.526376Z"
    },
    "id": "4udOR8OcfvME",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# =========================\n",
    "# Meta-classifier data preparation\n",
    "# =========================\n",
    "\n",
    "# Defining the path variables\n",
    "repeats = ['repeat 0/', 'repeat 1/', 'repeat 2/'] \n",
    "# repeats = ['repeat 0/']\n",
    "folds = ['fold 0/', 'fold 1/', 'fold 2/', 'fold 3/', 'fold 4/']\n",
    "model_names = ['vgg16', 'vgg19', 'resnet', 'densenet']\n",
    "path_base = '/kaggle/working/'\n",
    "\n",
    "# Expected column names for final meta-classifier input\n",
    "col_names = (\n",
    "    ['labels'] +\n",
    "    [f'vgg16_{i}' for i in range(5)] +\n",
    "    [f'vgg19_{i}' for i in range(5)] +\n",
    "    [f'resnet_{i}' for i in range(5)] +\n",
    "    [f'densenet_{i}' for i in range(5)]\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Iterating through each CV config\n",
    "# =========================\n",
    "\n",
    "for repeat in repeats:\n",
    "    for fold in folds:\n",
    "\n",
    "        train_list, test_list = [], []\n",
    "\n",
    "        # -------------------------\n",
    "        # Load and rename model predictions\n",
    "        # -------------------------\n",
    "        for model_name in model_names:\n",
    "\n",
    "            train_file = f\"{path_base}{repeat}{fold}{model_name}_train.csv\"\n",
    "            test_file = f\"{path_base}{repeat}{fold}{model_name}_test.csv\"\n",
    "\n",
    "            if not (os.path.exists(train_file) and os.path.exists(test_file)):\n",
    "                print(f\"Warning: Missing files for {model_name} in {repeat}{fold}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            df_train = pd.read_csv(train_file)\n",
    "            df_test = pd.read_csv(test_file)\n",
    "\n",
    "            # Rename probability columns to avoid collisions\n",
    "            prob_cols = [c for c in df_train.columns if c != 'labels']\n",
    "\n",
    "            df_train = df_train.rename(\n",
    "                columns={c: f\"{model_name}_{c}\" for c in prob_cols}\n",
    "            )\n",
    "            df_test = df_test.rename(\n",
    "                columns={c: f\"{model_name}_{c}\" for c in prob_cols}\n",
    "            )\n",
    "\n",
    "            train_list.append(df_train)\n",
    "            test_list.append(df_test)\n",
    "\n",
    "        # Skip if no data loaded\n",
    "        if len(train_list) == 0 or len(test_list) == 0:\n",
    "            print(f\"Skipping {repeat}{fold} due to missing files.\")\n",
    "            continue\n",
    "\n",
    "        # -------------------------\n",
    "        # Merge predictions from different models\n",
    "        # -------------------------\n",
    "        mc_train = train_list[0]\n",
    "        mc_test = test_list[0]\n",
    "\n",
    "        for i in range(1, len(train_list)):\n",
    "            mc_train = mc_train.merge(\n",
    "                train_list[i].drop(columns=['labels']),\n",
    "                left_index=True,\n",
    "                right_index=True\n",
    "            )\n",
    "            mc_test = mc_test.merge(\n",
    "                test_list[i].drop(columns=['labels']),\n",
    "                left_index=True,\n",
    "                right_index=True\n",
    "            )\n",
    "\n",
    "        # -------------------------\n",
    "        # Final sanity checks\n",
    "        # -------------------------\n",
    "        assert mc_train.shape[1] == len(col_names), \\\n",
    "            f\"Column mismatch in train: {mc_train.shape[1]} vs {len(col_names)}\"\n",
    "        assert mc_test.shape[1] == len(col_names), \\\n",
    "            f\"Column mismatch in test: {mc_test.shape[1]} vs {len(col_names)}\"\n",
    "\n",
    "        # -------------------------\n",
    "        # Assign final column names\n",
    "        # -------------------------\n",
    "        mc_train.columns = col_names\n",
    "        mc_test.columns = col_names\n",
    "\n",
    "        # -------------------------\n",
    "        # Save final meta-classifier inputs\n",
    "        # -------------------------\n",
    "        mc_train.to_csv(f\"{path_base}{repeat}{fold}mc_train_inputs.csv\", index=False)\n",
    "        mc_test.to_csv(f\"{path_base}{repeat}{fold}mc_test_inputs.csv\", index=False)\n",
    "\n",
    "        print(f\"Saved meta-classifier inputs for {repeat}{fold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-14T13:08:18.527667Z",
     "iopub.status.idle": "2026-01-14T13:08:18.528024Z",
     "shell.execute_reply": "2026-01-14T13:08:18.527859Z",
     "shell.execute_reply.started": "2026-01-14T13:08:18.527840Z"
    },
    "id": "CYEa3lWcgQBp",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training the meta-classifier\n",
    "\n",
    "# Defining path variables (for saving the predictions)\n",
    "repeats = ['repeat 0/', 'repeat 1/', 'repeat 2/']\n",
    "# repeats = ['repeat 0/']\n",
    "folds = ['fold 0/', 'fold 1/', 'fold 2/', 'fold 3/', 'fold 4/']\n",
    "path_base = '/kaggle/working/'\n",
    "\n",
    "# Meta-classifier hyperparameters\n",
    "num_epochs, learn_rate, batch_size, momentum, weight_decay = 200, 7.801e-2, 47, 0.9855, 5.526e-2\n",
    "\n",
    "# This training loop operates differently to the Base-model one.\n",
    "#   This one will not resume from the most recent fit.\n",
    "#   As the meta-classifier trains very quickly, this should be fine.\n",
    "\n",
    "# Training the meta-classifier\n",
    "for repeat in repeats:\n",
    "  for fold in folds:\n",
    "    # Load data and convert to tensor dataset\n",
    "    ds_train = MyDataset(path_base + repeat + fold + 'mc_train_inputs.csv')\n",
    "    ds_test = MyDataset(path_base + repeat + fold + 'mc_test_inputs.csv')\n",
    "    # Define dataloaders\n",
    "    dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "    dl_test = DataLoader(ds_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Define the model\n",
    "    model = metaClassifier()\n",
    "\n",
    "    # Training\n",
    "    trainMetaClassifier(model=model,\n",
    "          num_epochs=num_epochs,\n",
    "          learn_rate=learn_rate,\n",
    "          momentum=momentum,\n",
    "          weight_decay=weight_decay,\n",
    "          dl_train=dl_train,\n",
    "          dl_test=dl_test)\n",
    "\n",
    "    # Generate predictions\n",
    "    df_preds_train, df_preds_test = generateMCPredictions(dl_train=dl_train,\n",
    "                                                          dl_test=dl_test,\n",
    "                                                          batch_size=batch_size)\n",
    "    # Saving predictions\n",
    "    train_path = path_base + repeat + fold + 'mc_train_outputs.csv'\n",
    "    test_path = path_base + repeat + fold + 'mc_test_outputs.csv'\n",
    "    df_preds_train.to_csv(train_path, index=False)\n",
    "    df_preds_test.to_csv(test_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-14T13:08:18.529622Z",
     "iopub.status.idle": "2026-01-14T13:08:18.529984Z",
     "shell.execute_reply": "2026-01-14T13:08:18.529825Z",
     "shell.execute_reply.started": "2026-01-14T13:08:18.529806Z"
    },
    "id": "3_tP_Yh8chKp",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "all_model_metrics_train = []\n",
    "all_model_metrics_test  = []\n",
    "\n",
    "repeats = ['repeat 0/', 'repeat 1/', 'repeat 2/']\n",
    "folds   = ['fold 0/', 'fold 1/', 'fold 2/', 'fold 3/', 'fold 4/']\n",
    "path_base = '/kaggle/working/'\n",
    "\n",
    "for r in repeats:\n",
    "    for k in folds:\n",
    "\n",
    "        mc_data_train = pd.read_csv(path_base + r + k + 'mc_train_outputs.csv')\n",
    "        mc_data_test  = pd.read_csv(path_base + r + k + 'mc_test_outputs.csv')\n",
    "\n",
    "        y_train = mc_data_train[\"labels\"]\n",
    "        y_test  = mc_data_test[\"labels\"]\n",
    "\n",
    "        # Argmax over class probabilities\n",
    "        y_pred_train = mc_data_train.iloc[:, 1:].idxmax(axis=1).astype(int)\n",
    "        y_pred_test  = mc_data_test.iloc[:, 1:].idxmax(axis=1).astype(int)\n",
    "\n",
    "        # Train metrics\n",
    "        all_model_metrics_train.append([\n",
    "            f1_score(y_train, y_pred_train, average=\"macro\"),\n",
    "            recall_score(y_train, y_pred_train, average=\"macro\"),\n",
    "            precision_score(y_train, y_pred_train, average=\"macro\"),\n",
    "            accuracy_score(y_train, y_pred_train)\n",
    "        ])\n",
    "\n",
    "        # Test metrics\n",
    "        all_model_metrics_test.append([\n",
    "            f1_score(y_test, y_pred_test, average=\"macro\"),\n",
    "            recall_score(y_test, y_pred_test, average=\"macro\"),\n",
    "            precision_score(y_test, y_pred_test, average=\"macro\"),\n",
    "            accuracy_score(y_test, y_pred_test)\n",
    "        ])\n",
    "\n",
    "# Convert to DataFrames\n",
    "col_names = ['f1-score', 'recall', 'precision', 'accuracy']\n",
    "\n",
    "df_train = pd.DataFrame(all_model_metrics_train, columns=col_names)\n",
    "df_test  = pd.DataFrame(all_model_metrics_test,  columns=col_names)\n",
    "\n",
    "# Mean ± std (sample std)\n",
    "mean_train = df_train.mean()\n",
    "std_train  = df_train.std(ddof=1)\n",
    "\n",
    "mean_test = df_test.mean()\n",
    "std_test  = df_test.std(ddof=1)\n",
    "\n",
    "# Format as percentages\n",
    "avg_metrics_train = pd.DataFrame({\n",
    "    col: [f\"{mean_train[col]*100:.2f} ± {std_train[col]*100:.2f}\"]\n",
    "    for col in col_names\n",
    "})\n",
    "\n",
    "avg_metrics_test = pd.DataFrame({\n",
    "    col: [f\"{mean_test[col]*100:.2f} ± {std_test[col]*100:.2f}\"]\n",
    "    for col in col_names\n",
    "})\n",
    "\n",
    "print(\"Train (mean ± std, %):\")\n",
    "display(avg_metrics_train)\n",
    "\n",
    "print(\"Test (mean ± std, %):\")\n",
    "display(avg_metrics_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8855135,
     "sourceId": 14415336,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
